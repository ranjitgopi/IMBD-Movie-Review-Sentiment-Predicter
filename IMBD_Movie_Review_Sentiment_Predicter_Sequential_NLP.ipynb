{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8432e56e",
   "metadata": {},
   "source": [
    "The objective of this project is to build a text classification model that analyses the customer's \n",
    "sentiments based on their reviews in the IMDB database. The model uses a complex deep learning model to build \n",
    "an embedding layer followed by a classification algorithm to analyse the sentiment of the customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7afa97",
   "metadata": {},
   "source": [
    "The Dataset of 50,000 movie reviews from IMDB, labelled by sentiment (positive/negative). \n",
    "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For \n",
    "convenience, the words are indexed by their frequency in the dataset, meaning the for that has index 1 is the most \n",
    "frequent word. Use the first 20 words from each review to speed up training, using a max vocabulary size of \n",
    "10,000. As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd522b",
   "metadata": {},
   "source": [
    "PROJECT OBJECTIVE: To Build a sequential NLP classifier which can use input text parameters to determine the \n",
    "customer sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b72466c5",
   "metadata": {},
   "source": [
    "1. Import and analyse the data set. \n",
    "Hint: - Use `imdb.load_data()` method\n",
    " - Get train and test set\n",
    " - Take 10000 most frequent words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af8cc2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "C:\\Users\\rgi4\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:159: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
      "C:\\Users\\rgi4\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\datasets\\imdb.py:160: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "vocab_size = 10000\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "301ef4c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (25000,)\n",
      "Shape of X_test: (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\",X_train.shape)\n",
    "print(\"Shape of X_test:\",X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1182ae75",
   "metadata": {},
   "source": [
    "2. Perform relevant sequence adding on the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20716ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#We will use padding to keep each review at 500 words\n",
    "maxlen=500\n",
    "\n",
    "X_train = pad_sequences(X_train, maxlen = maxlen, padding = 'pre')\n",
    "X_test =  pad_sequences(X_test, maxlen = maxlen, padding = 'pre')\n",
    "\n",
    "X = np.concatenate((X_train, X_test), axis = 0)\n",
    "y = np.concatenate((y_train, y_test), axis = 0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2 )\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da17336",
   "metadata": {},
   "source": [
    "3. Perform following data analysis: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d531850",
   "metadata": {},
   "source": [
    "• Print shape of features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35d1979e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items in X_train: 32000\n",
      "Number of unique words in X_train: 9999\n",
      " \n",
      "Number of items in X_test: 10000\n",
      "Number of unique words in X_test: 9997\n",
      " \n",
      "Number of items in X_validation: 8000\n",
      "Number of unique words in X_validation: 9988\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of items in X_train:\",len(X_train))\n",
    "print(\"Number of unique words in X_train:\",len(np.unique(np.hstack(X_train))))\n",
    "print(\" \")\n",
    "print(\"Number of items in X_test:\",len(X_test))\n",
    "print(\"Number of unique words in X_test:\",len(np.unique(np.hstack(X_test))))\n",
    "print(\" \")\n",
    "print(\"Number of items in X_validation:\",len(X_validation))\n",
    "print(\"Number of unique words in X_validation:\",len(np.unique(np.hstack(X_validation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d608fb7",
   "metadata": {},
   "source": [
    "• Print value of any one feature and it's label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9e3581a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed below\n"
     ]
    }
   ],
   "source": [
    "print(\"Performed below\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57988ea4",
   "metadata": {},
   "source": [
    "4. Decode the feature value to get original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c085977",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_review(review_index):\n",
    "    word_index = imdb.get_word_index()\n",
    "    reverse_index = dict([(value, key) for (key, value) in word_index.items()]) \n",
    "    review_decoded = \" \".join( [reverse_index.get(i - 3, \"#\") for i in X_test[review_index]] )\n",
    "\n",
    "    print(\"The movie review index is:\",review_index)\n",
    "    print(\"The movie review is:\",X_test[review_index])\n",
    "    print(\"The movie review decoded is:\", review_decoded) \n",
    "    print(\"The movie review sentiment as reported by reviewer is (0 for negative and 1 for positive):\",y_test[review_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f822663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The movie review index is: 0\n",
      "The movie review is: [   0    0    0    0    0    0    0    0    0    1    4  236 4457    9\n",
      "    6   22   15    9  981    8   30  253    5 1127   12 2880   33  399\n",
      "  199   10   10    9    6  194  500 2243   37    2  194  500  140  822\n",
      "    2    6 1169   29    9 3601   23  170   23   35 5144   19    6  604\n",
      "    7   84  587    6 3852  773 5413    6  860 1656    6  232   37  495\n",
      "   18   27 1169  773 3447    5   35 2045  232   37   47 6678   90   23\n",
      "  111  773    2    2    4  213    9    8 2078   51    9 2417    8   30\n",
      "    2 3593 4457   15  556    4  236 5144    8    4 1609   36   80   30\n",
      "  397    8   14    2 1609  656    6    2 2196   15 4136 6354  446    4\n",
      " 7879  103  397    8    4 1609   36  515  169    4    2   63  497    8\n",
      " 1258   21   27 1056  214    2 3447 1241 5190   15  473    8 2335    4\n",
      " 4457   32  367    8  763   12    8   27 1594    7 7764 1389  137   36\n",
      "   71  245    2    2   68 1250    5  304    4 7879  245   39   68 1250\n",
      "  725 3582    4  604 2503   15   36  203   30   11    4 1609 1207   74\n",
      "   36  873    5 1630   15   29   80  516    4    2   10   10    4   65\n",
      "    9   87   18    6 1067 1219   22    2    8    6    2 1609    9  142\n",
      "   15  186  100   66  593    4 1385    9  307    5   12  272   40    6\n",
      "  273  121 3933  100  131    2   10   10   82   13  423    4  105   11\n",
      "   14   22    9    4  801  194   58 2243   37  494    8   79  233   15\n",
      "  100   30    6    2  516   82   87   16    2    2  256   34    2    2\n",
      "    7    4    2   37   66  272    5 1421   40   35 2045    2 2354    9\n",
      "    4  801  668   37 2906  712   18    4  604    5  186    8   24 4881\n",
      "   11    4 5152 3447    9    4 1138 7045    7   37   47   27  650    7\n",
      "   27 1410  653   54   29    9   11    4 1358   19   90   10   10    4\n",
      "    2   11   14   22    9   31    7    4  118   11    6   22   12 1407\n",
      "    6  117   99   56  208   40 4279  127    5   12 3465   94 5544   38\n",
      "   12    9    6  232   11    6 1735   21    4 1735  272   52  262    4\n",
      "  419    5    4 5544    5    4    2  272   52    5   55  629   13   28\n",
      "  110  958    7   85  108  121 4457 3675  168   96  433    2   11   14\n",
      "   22  518    2    6    2 4413   40    2    5   85  211    2   40  711\n",
      " 1991   39  711 1991 2949    5  711 1991 1918 4279   38    6   87    2\n",
      "   10   10   50    9  958    7  206   11    4  236 4457    7  843  854\n",
      "    9    6   87  548  200    2    5    2   82    6   87  136  121    2\n",
      "    2  497    8    2    4    2   50   26   85   87  531   15   13 4283\n",
      "  202  245   25   28    8   67   18  624   10   10   13  386   14   22\n",
      "    8  316  106   12    5   25   80   24   30  685]\n",
      "The movie review decoded is: # # # # # # # # # # the last dinosaur is a film that is meant to be fun and exciting it succeeds at doing both br br is a big game hunter who # big game go figure # a company he is planning on going on an expedition with a group of people including a photographer named frankie a japanese scientist a guy who works for his company named chuck and an african guy who has aided him on many named # # the point is to study what is believed to be # rex dinosaur that killed the last expedition to the area they will be getting to this # area taking a # vehicle that travels underwater called the polar after getting to the area they soon find the # which tries to shoot but his gun gets # chuck immediately senses that wanted to hunt the dinosaur all along to add it to his collection of stuffed animals while they were away # # their camp and takes the polar away from their camp upon returning the group realizes that they may be in the area longer than they expected and states that he will kill the # br br the story is great for a science fiction film # to a # area is something that seems could really happen the scenery is beautiful and it looks like a place where dinosaurs could still # br br also i liked the characters in this film is the typical big time hunter who wants to get anything that could be a # kill also great was # # played by # # of the # who really looks and acts like an african # jackie is the typical female who causes problems for the group and seems to not belong in the wilderness chuck is the former employee of who has his view of his boss change when he is in the wild with him br br the # in this film is one of the best in a film it stands a little too up right like godzilla does and it drags its tail so it is a guy in a suit but the suit looks good especially the head and the tail and the # looks good and very scary i have seen plenty of other films where dinosaur suits look way worse # in this film sometimes # a # sounding like # and other times # like king kong from king kong escapes and king kong vs godzilla so a great # br br there is plenty of action in the last dinosaur of particular note is a great fight between # and # also a great scene where # # tries to # the # there are other great parts that i wont give away you have to see for yourself br br i recommend this film to everyone watch it and you will not be disappointed\n",
      "The movie review sentiment as reported by reviewer is (0 for negative and 1 for positive): 1\n"
     ]
    }
   ],
   "source": [
    "decode_review(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195633e5",
   "metadata": {},
   "source": [
    "5. Design, train, tune and test a sequential NLP model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "808a04d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 500, 256)          2560000   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 500, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d (Conv1D)              (None, 500, 256)          327936    \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 500, 128)          163968    \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 166, 128)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 166, 64)           41024     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 55, 64)            0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 75)                42000     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 76        \n",
      "=================================================================\n",
      "Total params: 3,135,004\n",
      "Trainable params: 3,135,004\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, MaxPooling1D, Conv1D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "\n",
    "#We will use Long Short Term Memory (LSTM)\n",
    "# Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 256, input_length = maxlen))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv1D(256, 5, padding = 'same', activation = 'relu'))\n",
    "model.add(Conv1D(128, 5, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 3))\n",
    "model.add(Conv1D(64, 5, padding = 'same', activation = 'relu'))\n",
    "model.add(MaxPooling1D(pool_size = 3))\n",
    "model.add(LSTM(75))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2de276a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "500/500 [==============================] - 275s 550ms/step - loss: 0.3474 - accuracy: 0.8294 - val_loss: 0.2692 - val_accuracy: 0.8891\n",
      "Epoch 2/10\n",
      "500/500 [==============================] - 264s 527ms/step - loss: 0.1735 - accuracy: 0.9349 - val_loss: 0.2535 - val_accuracy: 0.9049\n",
      "Epoch 3/10\n",
      "500/500 [==============================] - 285s 571ms/step - loss: 0.1160 - accuracy: 0.9590 - val_loss: 0.3699 - val_accuracy: 0.8771\n",
      "Epoch 4/10\n",
      "500/500 [==============================] - 263s 525ms/step - loss: 0.0752 - accuracy: 0.9756 - val_loss: 0.3766 - val_accuracy: 0.8826\n",
      "Epoch 5/10\n",
      "500/500 [==============================] - 262s 525ms/step - loss: 0.0523 - accuracy: 0.9833 - val_loss: 0.3683 - val_accuracy: 0.9015\n",
      "Epoch 6/10\n",
      "500/500 [==============================] - 275s 551ms/step - loss: 0.0400 - accuracy: 0.9868 - val_loss: 0.3740 - val_accuracy: 0.8961\n",
      "Epoch 7/10\n",
      "500/500 [==============================] - 268s 537ms/step - loss: 0.0287 - accuracy: 0.9911 - val_loss: 0.4505 - val_accuracy: 0.8955\n",
      "Epoch 8/10\n",
      "500/500 [==============================] - 265s 530ms/step - loss: 0.0239 - accuracy: 0.9929 - val_loss: 0.4338 - val_accuracy: 0.8903\n",
      "Epoch 9/10\n",
      "500/500 [==============================] - 269s 538ms/step - loss: 0.0200 - accuracy: 0.9938 - val_loss: 0.4723 - val_accuracy: 0.8878\n",
      "Epoch 10/10\n",
      "500/500 [==============================] - 276s 552ms/step - loss: 0.0183 - accuracy: 0.9944 - val_loss: 0.4594 - val_accuracy: 0.8945\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x28d0d830c40>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit model\n",
    "model.fit(X_train, y_train, validation_data = (X_validation, y_validation), epochs = 10, batch_size = 64, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07829f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-caa8f3476c2b>:3: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "Classification Report:               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.90      0.89      4920\n",
      "           1       0.90      0.89      0.90      5080\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_prediction = model.predict_classes(X_test)\n",
    "print(\"Classification Report:\", classification_report(y_prediction, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586d3fee",
   "metadata": {},
   "source": [
    "6. Use the designed model to print the prediction on any one sample.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0637f6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing...\n",
      "The movie review index is: 5797\n",
      "The movie review is: [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    1   48   25  181    8\n",
      "   66    2   84 2257    4 2113   17  129  875  103   32    6   52  855\n",
      "    7    4 3895  264   15   29    9  147 1617   25   26    2   83    6\n",
      "  226    2    2    7 1751 7433 1736    5    2 2656    5 2725    8 1203\n",
      "  179    6  171 4058   11  129  185  311   10   10   14  390   69   32\n",
      "    4 1267    7    6  363 2645   11  192  260 1769  149   12   13    2\n",
      "  120    8  160 1308   63   16  395    6 1136 1648   22    5  179 2035\n",
      "  426 3259    6 1153  200    4 3855   11    2   19    6 3722  177    2\n",
      "   34   94  904   23    6  834 1709  601   19 3981    2 2692    7   35\n",
      " 2518   12   16   17 5773    5 1596   17 1536   42 1494    4  328 2944\n",
      " 1005   93   12  235   17   48    4  328 3324    7  465  551   16    2\n",
      "    4 1709 1646    4 1225    5    8  845   16 9659 1004    5   17    4\n",
      "  189    9    2   34    4 1857   15    4  884    5 2313   26 6384    4\n",
      " 2950    2  380    2    6    2  544    5   95    4  105  380  112 1637\n",
      "  125   31   34   31   10   10   14   47   32    4  118 2432    7    4\n",
      " 5234    7   14  162  884   37    2  250   11    4    2    5    4 1896\n",
      "  506 2148 1728 2532    5 1984   32  209 5981  954   14    9  230 8538\n",
      "   74  101 1166    7 1549    2   21   12   47    8   30 1803    9   14\n",
      "   66 4513  829   18  476   14    9   24    6  223 2575  390 2240    4\n",
      "  338 4069   37   47    8  276   68  362    8 1445  103   14   31  146\n",
      "   24  252   13   60  181    8   67   51  571  375]\n",
      "The movie review decoded is: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # if you want to really # people choose the devil as your subject after all a good deal of the population believe that he is real therefore you are # into a whole # # of pre existing religious and # imagery and bound to cause quite a few nightmares in your young audience br br this episode had all the appearance of a hollywood blockbuster in fact having finished watching it i # over to another channel which was playing a recent bond film and quite frankly couldn't split a hair between the differences in # with a minimal cast # by its situation on a space station complete with overwhelming # views of an universe it was as claustrophobic and intense as alien or event the black hole outside made it feel as if the black weight of dark matter was # the station onto the planet and to whatever was sealed inside and as the horror is # by the knowledge that the dr and rose are stranded the sinister # start # a # voice and then the characters start being picked off one by one br br this has all the best qualities of the cream of this new dr who # girl in the # and the empty child emotionally engaging frightening and humorous all without seeming cheesy this is far scarier than any amount of flying # but it has to be asked is this really suitable viewing for children this is not a family friendly episode pity the poor parent who has to put their kids to bed after this one i'm not sure i even want to see what happens next\n",
      "The movie review sentiment as reported by reviewer is (0 for negative and 1 for positive): 1\n",
      "Predicted sentiment of user by model: 1\n"
     ]
    }
   ],
   "source": [
    "print('testing...')\n",
    "\n",
    "import random\n",
    "review_index=random.randint(0,10000)\n",
    "\n",
    "decode_review(review_index)\n",
    "print('Predicted sentiment of user by model:', y_prediction[review_index][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1e1099",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
